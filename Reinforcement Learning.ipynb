{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37b9be5-51ee-4579-9912-0b97b3e10bc0",
   "metadata": {},
   "source": [
    "# AI Reinforcement Learning Crash Course - Overview\n",
    "\n",
    "## Introduction to AI\n",
    "Artificial Intelligence (AI) is the field of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. These include problem-solving, pattern recognition, decision-making, and learning. \n",
    "\n",
    "AI is classified into:\n",
    "- **Narrow AI**: Specialized in one task, like recommendation systems.\n",
    "- **General AI**: Hypothetical systems with human-like cognitive abilities.\n",
    "\n",
    "## Neural Networks\n",
    "Neural networks are computational models inspired by the human brain. They consist of layers of nodes (neurons) connected by weighted edges, which adjust during training. \n",
    "\n",
    "While they can be represented as **network flow graphs**, they are more than just an application of themâ€”they involve **non-linear transformations** and **backpropagation** for learning.\n",
    "\n",
    "## Types of Training\n",
    "### **1. Supervised Learning**\n",
    "- Trained on **labeled** data (input-output pairs).\n",
    "- Learns by minimizing the error between predictions and actual outcomes.\n",
    "\n",
    "### **2. Unsupervised Learning**\n",
    "- Trained on **unlabeled** data.\n",
    "- Tries to find **patterns**, such as clustering similar data points.\n",
    "\n",
    "### **3. Reinforcement Learning (RL)**\n",
    "- The model (**agent**) learns by interacting with an **environment**.\n",
    "- Receives **rewards** for taking actions that maximize long-term benefits.\n",
    "\n",
    "## Difference Between Supervised and Unsupervised Learning\n",
    "| **Type**        | **Training Data** | **Goal** |\n",
    "|----------------|----------------|---------|\n",
    "| Supervised    | Labeled data (input-output pairs) | Learn to map inputs to outputs |\n",
    "| Unsupervised  | Unlabeled data | Find hidden patterns or structures |\n",
    "\n",
    "## Reinforcement Learning Terms (**AREA 51**) \n",
    "- **Agent**: The learner or decision-maker.\n",
    "- **Environment**: The world in which the agent interacts.\n",
    "- **Action**: Choices the agent can make.\n",
    "- **Reward**: Feedback from the environment indicating the quality of an action.\n",
    "\n",
    "- **State**: The current situation of the agent.\n",
    "- **Policy**: The strategy the agent uses to decide actions.\n",
    "\n",
    "## What is a Heuristic?\n",
    "A **heuristic** is a problem-solving approach that relies on practical shortcuts or **rules of thumb** to make decisions more efficiently. In reinforcement learning, heuristics are sometimes used to define rewards in a way that encourages desirable behavior even if an optimal policy is not yet learned.\n",
    "\n",
    "## Tools You'll Be Using\n",
    "This is based off [Nicholas Renotte's workshop](https://www.youtube.com/watch?v=cO5g5qLrLSo&ab_channel=NicholasRenotte) on Deep Learning. We'll be creating an agent that can balance a stick. Then we'll design our own\n",
    "### **1. OpenAI Gym**\n",
    "- A toolkit for developing and testing RL algorithms with various simulated environments.\n",
    "\n",
    "### **2. TensorFlow**\n",
    "- A machine learning framework optimized for deep learning and neural networks.\n",
    "\n",
    "### **3. Keras-RL Agents**\n",
    "- A library for reinforcement learning that integrates with **Keras** and **TensorFlow**, providing pre-built RL algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca72010-e632-446c-99d8-db088189cff1",
   "metadata": {},
   "source": [
    "# **0. Install Dependencies**\n",
    "This is an up-to-date workshop based off [Nicholas Renotte's workshop](https://www.youtube.com/watch?v=cO5g5qLrLSo&ab_channel=NicholasRenotte) on Deep Learning. We'll be creating an agent that can balance a stick. Let's first install our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b6ae09-59d2-42e9-8fc0-bc95b8a95b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.25.2 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym==0.25.2) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym==0.25.2) (1.3.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym==0.25.2) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.8.0->gym==0.25.2) (3.20.2)\n",
      "Requirement already satisfied: keras==2.10.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: keras-rl2==1.0.5 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras-rl2==1.0.5) (2.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.20.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.17.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (41.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.3.0->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
      "Requirement already satisfied: protobuf==3.20.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.20.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: gym[classic_control] in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym[classic_control]) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym[classic_control]) (1.3.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym[classic_control]) (8.5.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym[classic_control]) (2.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\tuber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.25.2\n",
    "!pip install keras==2.10.0\n",
    "!pip install keras-rl2==1.0.5\n",
    "!pip install protobuf==3.20.0\n",
    "!pip install numpy\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3cbd5-6926-4e8d-9cdb-8e7533e4d55c",
   "metadata": {},
   "source": [
    "**If you have any problems installing tensorflow**, make sure pip is pointing to pip3 because tensorflow requires python3. In particular python 3.8\n",
    "\n",
    "`pip --version`\n",
    "\n",
    "`pip3 install --upgrade tensorflow`\n",
    "\n",
    "https://stackoverflow.com/questions/63073711/how-to-install-tensorflow-2-3-0\n",
    "\n",
    "Try also running this notebook in vscode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b8c5c-803b-42cc-95ed-ed73e2d6adaa",
   "metadata": {},
   "source": [
    "# **1. Introduction to CartPole in OpenAI Gym**\n",
    "\n",
    "## **What is CartPole?**\n",
    "CartPole is a classic reinforcement learning environment provided by **OpenAI Gym**. It is a simple physics-based control problem where an agent must balance a pole on top of a moving cart.\n",
    "\n",
    "## **Objective**\n",
    "The goal is to keep the pole balanced for as long as possible by moving the cart **left** or **right**. The agent receives a **reward** for each time step the pole remains upright.\n",
    "\n",
    "## **What is an Episode in Reinforcement Learning?**\n",
    "An **episode** is a single run of the environment from **start to termination**. In CartPole, an episode:\n",
    "- **Starts** with the cart and pole in a random initial state.\n",
    "- **Continues** as the agent takes actions and receives rewards.\n",
    "- **Ends** when the termination conditions are met (e.g., the pole falls or the cart moves out of bounds).\n",
    "\n",
    "Each episode provides training data for the reinforcement learning agent to improve its policy.\n",
    "\n",
    "## **State Variables (Observation Space)**\n",
    "The environment has **4 continuous state variables**:\n",
    "1. **Cart Position** - Horizontal position of the cart.\n",
    "2. **Cart Velocity** - Speed of the cart.\n",
    "3. **Pole Angle** - Angle of the pole relative to vertical.\n",
    "4. **Pole Angular Velocity** - Rotational speed of the pole.\n",
    "\n",
    "## **Action Space**\n",
    "The agent has **2 discrete actions**:\n",
    "- `0`: Push the cart **left**.\n",
    "- `1`: Push the cart **right**.\n",
    "\n",
    "## **Rewards**\n",
    "- The agent **receives +1 reward** for each time step the pole stays balanced.\n",
    "- The episode **ends when**:\n",
    "  - The pole falls **more than 15 degrees** from vertical.\n",
    "  - The cart moves **too far** from the center.\n",
    "\n",
    "## **Official OpenAI Gym Documentation**\n",
    "For more details, visit:\n",
    "[ðŸ”— CartPole-v1 Documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "Let's begin to setup the enviorment. Keep in mind you may need up-to-date versions or installations of `pip`, `pygame`, `numpy`, and or other dependencies if errors arise.\n",
    "\n",
    "The enviroment opens a new window, if you don't see it, look in your tab or other displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60966a90-a65d-4f6c-a050-69c50afa6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym    # OpenAI's gym\n",
    "import random # To start we'll take random steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc5bec4-aedc-4338-88b7-0d9463f787ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9ce89a-20ac-4b83-8b5d-d9b6d328a8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:  4\n",
      "Actions:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"States: \", states)\n",
    "print(\"Actions: \", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33957397-f8f4-444e-a7d4-f0f62e2279db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 18.0\n",
      "Episode: 2 Score: 13.0\n",
      "Episode: 3 Score: 13.0\n",
      "Episode: 4 Score: 22.0\n",
      "Episode: 5 Score: 19.0\n",
      "Episode: 6 Score: 12.0\n",
      "Episode: 7 Score: 23.0\n",
      "Episode: 8 Score: 12.0\n",
      "Episode: 9 Score: 23.0\n",
      "Episode: 10 Score: 22.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()  # Unpacking reset output\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1])  # Randomly choose between 0 & 1\n",
    "        n_state, reward, done, info = env.step(action)  \n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score: {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12adf4-2fb1-4147-84b3-464c292de209",
   "metadata": {},
   "source": [
    "# **Gym API Guide - Key Functions**\n",
    "\n",
    "## **1. `gym.make()` - Create an Environment**\n",
    "```python\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "```\n",
    "- Initializes an environment.\n",
    "- `render_mode=\"human\"` â†’ Displays the environment.\n",
    "- `render_mode=\"rgb_array\"` â†’ Returns an image instead.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. State and Action Spaces**\n",
    "```python\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "```\n",
    "- **`env.observation_space.shape[0]`** â†’ Number of state variables.\n",
    "- **`env.action_space.n`** â†’ Number of possible actions.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. `env.reset()` - Reset the Environment**\n",
    "```python\n",
    "state, info = env.reset()\n",
    "```\n",
    "- **Returns:** `(state, info)`\n",
    "  - `state`: The initial observation (environment state).\n",
    "  - `info`: Extra environment metadata (not always needed).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. `env.step(action)` - Take an Action**\n",
    "```python\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "```\n",
    "- **Expects:** `action` (an integer representing the chosen action).\n",
    "- **Returns:**\n",
    "  - `next_state`: The new state after the action.\n",
    "  - `reward`: Reward received for taking the action.\n",
    "  - `terminated`: `True` if the episode ended (e.g., pole fell).\n",
    "  - `truncated`: `True` if the episode was forcefully stopped.\n",
    "  - `info`: Additional debugging info.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. `env.render()` - Display Environment**\n",
    "```python\n",
    "env.render()\n",
    "```\n",
    "- Renders the environment in a new window when `render_mode=\"human\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. `random.choice()` - Select Random Action**\n",
    "```python\n",
    "import random\n",
    "action = random.choice([0, 1])\n",
    "```\n",
    "- Randomly selects an action from a given list.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "| Function               | Purpose |\n",
    "|------------------------|---------|\n",
    "| `gym.make()`          | Creates the environment. |\n",
    "| `env.observation_space` | Gets the number of state variables. |\n",
    "| `env.action_space`     | Gets the number of available actions. |\n",
    "| `env.reset()`         | Resets the environment and returns the initial state. |\n",
    "| `env.step(action)`    | Takes an action and returns the new state, reward, and termination info. |\n",
    "| `env.render()`        | Displays the environment visually. |\n",
    "| `random.choice()`     | Randomly selects an action. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513de4a-15f0-4880-8560-631f6739d39b",
   "metadata": {},
   "source": [
    "# **2. Creating a Deep Learning Model with Keras**\n",
    "## **What Does \"Deep\" Mean?**\n",
    "- \"Deep Learning\" refers to models that consist of **multiple layers** of artificial neurons.\n",
    "- The term \"deep\" comes from **stacking many layers** in a neural network.\n",
    "\n",
    "## **Deep vs. Shallow Learning**\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| **Shallow Learning** | Uses fewer layers, relies on manual feature selection (e.g., decision trees, logistic regression). |\n",
    "| **Deep Learning** | Uses multiple hidden layers, learns complex features automatically. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58973cd2-18a2-49fb-83da-c8c9e60d58e6",
   "metadata": {},
   "source": [
    "## **Understanding Neural Network Architectures**\n",
    "\n",
    "### **1. Sequential Neural Networks**\n",
    "\n",
    "A **Sequential Neural Network** is a type of artificial neural network where data flows in one directionâ€”from input to outputâ€”through a series of layers. Each layer's output serves as the input for the next layer. This architecture is straightforward and commonly used for problems where data transformations are linear and order-dependent.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Linear Stack of Layers:** Each layer has one input and one output.\n",
    "- **Simplified Design:** Easy to implement and understand.\n",
    "- **Limitations:** Not ideal for tasks requiring memory of previous inputs or hierarchical data processing.\n",
    "\n",
    "### **Choosing or Designing a Model**\n",
    "\n",
    "When selecting a neural network architecture, one must first consider the specific use case. Different tasks, such as image recognition, natural language processing, or time-series prediction, require different model architectures. In some cases, an existing model may be sufficient, while in others, a **custom model** may need to be developed.\n",
    "\n",
    "Popular architectures include **Convolutional Neural Networks (CNNs)** for images, **Recurrent Neural Networks (RNNs)** for sequences, **Transformers** for NLP (Natural Language Processing), and **Generative Adversarial Networks (GANs)** for data generation. Each model type is specialized for different tasks.\n",
    "\n",
    "### **Creating a Custom Model**\n",
    "Designing a neural network from scratch involves:\n",
    "- **Defining the Problem:** Understanding the type of data and the expected outputs.\n",
    "- **Choosing the Right Layers:** Deciding how many layers and what types (e.g., convolutional, recurrent, dense) are needed.\n",
    "- **Selecting an Activation Function:** Using appropriate functions like ReLU, sigmoid, or softmax based on the task.\n",
    "- **Tuning Hyperparameters:** Adjusting learning rates, batch sizes, and number of neurons to optimize performance.\n",
    "- **Testing and Iterating:** Running experiments, analyzing results, and refining the architecture based on performance.\n",
    "\n",
    "Developers often use **transfer learning** (leveraging pre-trained models) to save time and computational resources rather than building models from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0d5a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import Keras modules for building a deep learning model\n",
    "from tensorflow.keras.models import Sequential  # Sequential model for stacking layers\n",
    "from tensorflow.keras.layers import Dense, Flatten  # Layers for building the neural network\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "254dc623-1774-44e0-9ea0-b070a52ab956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    \"\"\"\n",
    "    Builds a deep learning model for reinforcement learning.\n",
    "\n",
    "    Parameters:\n",
    "    states (int): Number of state variables (input features).\n",
    "    actions (int): Number of possible actions (output size).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): A compiled neural network model.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Initialize a sequential neural network\n",
    "    \n",
    "    # Layer 1: Flatten layer (Not trainable, but still a layer)\n",
    "    # Converts input into a 1D vector for Dense layers to process.\n",
    "    model.add(Flatten(input_shape=(1, states)))  \n",
    "    \n",
    "    # Layer 2: First hidden layer with 24 neurons\n",
    "    # Dense (fully connected) means every neuron is connected to all neurons in the next layer\n",
    "    # ReLU (Rectified Linear Unit) activation introduces non-linearity:\n",
    "    #   - f(x) = max(0, x), meaning negative values are set to 0\n",
    "    #   - Helps prevent vanishing gradients and improves training stability\n",
    "    model.add(Dense(24, activation='relu'))  \n",
    "    \n",
    "    # Layer 3: Second hidden layer with 24 neurons and ReLU activation\n",
    "    # Stacking layers allows the network to learn more complex patterns\n",
    "    model.add(Dense(24, activation='relu'))  \n",
    "    \n",
    "    # Layer 4: Output layer with 'actions' neurons, using a linear activation function\n",
    "    # Linear activation is used because this is a regression task (Q-values)\n",
    "    # Q-values represent the expected future reward for each action, so no constraints (like softmax) are applied\n",
    "    model.add(Dense(actions, activation='linear'))  \n",
    "    \n",
    "    return model  # Return the compiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79969ed8-d761-498b-a43e-2b4a9d2f2f27",
   "metadata": {},
   "source": [
    "More layers **do not always mean better performance**â€”it's something that must be **experimented with and fine-tuned** based on the problem and dataset.\n",
    "\n",
    "### **When More Layers Help:**\n",
    "**Complex Patterns** â€“ Deep networks capture hierarchical features (e.g., CNNs for images).  \n",
    "**Large Datasets** â€“ More data allows deeper models to generalize better.  \n",
    "**Sequential Data** â€“ Tasks like NLP or time-series forecasting benefit from deep architectures.  \n",
    "\n",
    "### **When More Layers Hurt:**\n",
    "**Overfitting** â€“ Too many layers can cause memorization instead of generalization.  \n",
    "**Vanishing/Exploding Gradients** â€“ Training deep networks can be unstable without proper techniques (e.g., batch normalization, skip connections).  \n",
    "**Computational Cost** â€“ More layers require more training time and computational power.  \n",
    "\n",
    "### **How to Find the Right Depth?**\n",
    "**Start Simple** â€“ Use a shallow network and gradually increase layers.  \n",
    "**Experiment** â€“ Adjust the number of layers based on validation performance.  \n",
    "**Use Regularization** â€“ Techniques like dropout or weight decay can help prevent overfitting in deep networks.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cad17b6-c54b-4b4a-b017-1808af3348f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "821b47f3-dc91-4116-8390-db9f04015989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # Displays a summary of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa038d-f002-472c-974f-59a6d9c10a2c",
   "metadata": {},
   "source": [
    "# **3. Build Agent with Keras-RL**\n",
    "Now that we have our model, we breathe life into it via our Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e148d17",
   "metadata": {},
   "source": [
    "## **Policy-Based vs. Value-Based Reinforcement Learning**\n",
    "\n",
    "### **Value-Based Methods**\n",
    "\n",
    "- **Focus on learning**: The value (expected return) of being in a state or taking an action in a state\n",
    "- **Decision making**: Choose actions with the highest estimated value\n",
    "- **Examples**: Q-Learning, DQN (Deep Q-Networks)\n",
    "- **Key idea**: \"How good is this state or action?\"\n",
    "\n",
    "### **Policy-Based Methods**\n",
    "- **Focus on learning**: The policy (mapping from states to actions) directly\n",
    "- **Decision making**: Sample actions from the learned probability distribution\n",
    "- **Examples**: REINFORCE, Proximal Policy Optimization (PPO)\n",
    "- **Key idea**: \"What action should I take in this state?\"\n",
    "\n",
    "## **Agents in Keras-RL**\n",
    "\n",
    "In the context of our model, an **agent** is the decision-making entity that:\n",
    "1. Observes the environment state\n",
    "2. Chooses actions based on a strategy\n",
    "3. Learns from experience to improve its decision-making\n",
    "\n",
    "Keras-RL provides several agent implementations:\n",
    "\n",
    "- **DQNAgent**: Deep Q-Network agent. Uses neural networks to approximate Q-values (state-action values), combining Q-learning with deep learning.\n",
    "- **DDPGAgent**: Deep Deterministic Policy Gradient agent. Useful for continuous action spaces.\n",
    "- **ContinuousDQNAgent**: Adaptation of DQN for continuous action spaces.\n",
    "- **CEMAgent**: Cross-Entropy Method agent. Uses an evolution strategy for optimization.\n",
    "- **SARSAAgent**: State-Action-Reward-State-Action agent. Similar to Q-learning but uses the current policy to select the next action.\n",
    "\n",
    "## **Why We're Using DQNAgent**\n",
    "\n",
    "For our CartPole problem, we're using the **DQNAgent** because:\n",
    "- It's well-suited for discrete action spaces (like our left/right choices)\n",
    "- It combines deep learning (our neural network) with Q-learning\n",
    "- It includes experience replay (SequentialMemory) to improve sample efficiency\n",
    "- It's stable and has proven effective for control tasks like CartPole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7d8855-651d-4e8b-9a4c-c9165f8dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent # Import DQN agent for reinforcement learning\n",
    "from rl.policy import BoltzmannQPolicy  # Import Boltzmann policy for action selection based on Q-values\n",
    "from rl.memory import SequentialMemory # Sequential memory for experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "627e31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()  # Boltzmann policy for action selection\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)  # Sequential memory for experience replay\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94cb5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape: 4\n"
     ]
    }
   ],
   "source": [
    "# Check the observation space shape\n",
    "print(\"Observation space shape:\", env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86f0bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 8:11 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 70s 7ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 198.680 [134.000, 200.000] - loss: 17.741 - mae: 47.677 - mean_q: 95.463\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.0000\n",
      "63 episodes - episode_reward: 159.063 [40.000, 200.000] - loss: 17.816 - mae: 44.624 - mean_q: 89.194\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 1.0000\n",
      "53 episodes - episode_reward: 187.925 [38.000, 200.000] - loss: 13.905 - mae: 40.726 - mean_q: 81.458\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 197.920 [96.000, 200.000] - loss: 16.084 - mae: 42.316 - mean_q: 84.747\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 1.0000\n",
      "done, took 346.620 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1332d522e50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)  # Build the DQN agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])  # Compile the DQN agent\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)  # Fit the DQN agent to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a933714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n",
      "Episode 21: reward: 200.000, steps: 200\n",
      "Episode 22: reward: 200.000, steps: 200\n",
      "Episode 23: reward: 200.000, steps: 200\n",
      "Episode 24: reward: 200.000, steps: 200\n",
      "Episode 25: reward: 200.000, steps: 200\n",
      "Episode 26: reward: 200.000, steps: 200\n",
      "Episode 27: reward: 200.000, steps: 200\n",
      "Episode 28: reward: 200.000, steps: 200\n",
      "Episode 29: reward: 200.000, steps: 200\n",
      "Episode 30: reward: 200.000, steps: 200\n",
      "Episode 31: reward: 200.000, steps: 200\n",
      "Episode 32: reward: 200.000, steps: 200\n",
      "Episode 33: reward: 200.000, steps: 200\n",
      "Episode 34: reward: 200.000, steps: 200\n",
      "Episode 35: reward: 200.000, steps: 200\n",
      "Episode 36: reward: 200.000, steps: 200\n",
      "Episode 37: reward: 200.000, steps: 200\n",
      "Episode 38: reward: 200.000, steps: 200\n",
      "Episode 39: reward: 200.000, steps: 200\n",
      "Episode 40: reward: 200.000, steps: 200\n",
      "Episode 41: reward: 200.000, steps: 200\n",
      "Episode 42: reward: 200.000, steps: 200\n",
      "Episode 43: reward: 200.000, steps: 200\n",
      "Episode 44: reward: 200.000, steps: 200\n",
      "Episode 45: reward: 200.000, steps: 200\n",
      "Episode 46: reward: 200.000, steps: 200\n",
      "Episode 47: reward: 200.000, steps: 200\n",
      "Episode 48: reward: 200.000, steps: 200\n",
      "Episode 49: reward: 200.000, steps: 200\n",
      "Episode 50: reward: 200.000, steps: 200\n",
      "Episode 51: reward: 200.000, steps: 200\n",
      "Episode 52: reward: 200.000, steps: 200\n",
      "Episode 53: reward: 200.000, steps: 200\n",
      "Episode 54: reward: 200.000, steps: 200\n",
      "Episode 55: reward: 200.000, steps: 200\n",
      "Episode 56: reward: 200.000, steps: 200\n",
      "Episode 57: reward: 200.000, steps: 200\n",
      "Episode 58: reward: 200.000, steps: 200\n",
      "Episode 59: reward: 200.000, steps: 200\n",
      "Episode 60: reward: 200.000, steps: 200\n",
      "Episode 61: reward: 200.000, steps: 200\n",
      "Episode 62: reward: 200.000, steps: 200\n",
      "Episode 63: reward: 200.000, steps: 200\n",
      "Episode 64: reward: 200.000, steps: 200\n",
      "Episode 65: reward: 200.000, steps: 200\n",
      "Episode 66: reward: 200.000, steps: 200\n",
      "Episode 67: reward: 200.000, steps: 200\n",
      "Episode 68: reward: 200.000, steps: 200\n",
      "Episode 69: reward: 200.000, steps: 200\n",
      "Episode 70: reward: 200.000, steps: 200\n",
      "Episode 71: reward: 200.000, steps: 200\n",
      "Episode 72: reward: 200.000, steps: 200\n",
      "Episode 73: reward: 200.000, steps: 200\n",
      "Episode 74: reward: 200.000, steps: 200\n",
      "Episode 75: reward: 200.000, steps: 200\n",
      "Episode 76: reward: 200.000, steps: 200\n",
      "Episode 77: reward: 200.000, steps: 200\n",
      "Episode 78: reward: 200.000, steps: 200\n",
      "Episode 79: reward: 200.000, steps: 200\n",
      "Episode 80: reward: 200.000, steps: 200\n",
      "Episode 81: reward: 200.000, steps: 200\n",
      "Episode 82: reward: 200.000, steps: 200\n",
      "Episode 83: reward: 200.000, steps: 200\n",
      "Episode 84: reward: 200.000, steps: 200\n",
      "Episode 85: reward: 200.000, steps: 200\n",
      "Episode 86: reward: 200.000, steps: 200\n",
      "Episode 87: reward: 200.000, steps: 200\n",
      "Episode 88: reward: 200.000, steps: 200\n",
      "Episode 89: reward: 200.000, steps: 200\n",
      "Episode 90: reward: 200.000, steps: 200\n",
      "Episode 91: reward: 200.000, steps: 200\n",
      "Episode 92: reward: 200.000, steps: 200\n",
      "Episode 93: reward: 200.000, steps: 200\n",
      "Episode 94: reward: 200.000, steps: 200\n",
      "Episode 95: reward: 200.000, steps: 200\n",
      "Episode 96: reward: 200.000, steps: 200\n",
      "Episode 97: reward: 200.000, steps: 200\n",
      "Episode 98: reward: 200.000, steps: 200\n",
      "Episode 99: reward: 200.000, steps: 200\n",
      "Episode 100: reward: 200.000, steps: 200\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)  # Test the DQN agent\n",
    "print(np.mean(scores.history['episode_reward']))  # Print the average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c7af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)  # Test the DQN agent with visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6994f6",
   "metadata": {},
   "source": [
    "# **4. Reloading Agent from Memory:**\n",
    "\n",
    "We can save our weights to use for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da8f1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09ee9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will delete all our variables to prove we can reload our model\n",
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27ffea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tuber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\tuber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')  # Create a new environment\n",
    "actions = env.action_space.n  # Get the number of possible actions\n",
    "states = env.observation_space.shape[0]  # Get the number of state variables\n",
    "model = build_model(states, actions)  # Build a new model\n",
    "dqn = build_agent(model, actions)  # Build a new agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])  # Compile the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b231e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')  # Load the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b45b11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1332ee3f8e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)  # Test the agent with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b21e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
